# ğŸ§  AI Customer Support Agent (RAG with Ollama + LangChain + FastAPI + Streamlit)

This project is an AI-powered customer support assistant built using:

- ğŸ¤– LangChain (RAG with FAISS & Ollama)
- ğŸ“„ PDF Knowledge Ingestion
- ğŸ”¥ FastAPI for backend API
- ğŸŒ Streamlit for web frontend

## ğŸš€ Features

- Upload PDF documents to build a custom knowledge base.
- Ask natural language questions about the uploaded content.
- Chat interface powered by Retrieval-Augmented Generation (RAG).
- Powered by local Ollama LLM (e.g. LLaMA2).
- Uses FAISS for vector store & semantic retrieval.

## ğŸ—ï¸ Project Structure

```bash
ai-customer-support-mcp/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                # FastAPI server
â”‚   â”œâ”€â”€ rag_pipeline.py        # Ingest PDF + create vectorstore + build RAG chain
â”‚   â”œâ”€â”€ agent.py               # Core chat agent logic
â”‚   â”œâ”€â”€ context_memory.py      # (Optional) Session memory (future use)
â”‚   â””â”€â”€ uploads/               # Uploaded PDFs (auto-created)
â”‚
â”œâ”€â”€ app.py                     # Streamlit frontend interface
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

![image](https://github.com/user-attachments/assets/17e6bb96-caad-45f9-9e36-2268ee14a339)
